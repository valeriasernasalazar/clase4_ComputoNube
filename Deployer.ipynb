{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el archivo para subir el modelo a la nube.\n",
    "\n",
    "\n",
    "CÓMO FUNCIONA:\n",
    "\n",
    "1. Tomar un modelo ya realizado.\n",
    "\n",
    "2. Lo sube a la nube.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model modelo\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Cargar el archivo config.json\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Crear el workspace\n",
    "ws = Workspace.create(\n",
    "    name=config[\"workspace_name\"],\n",
    "    subscription_id=config[\"subscription_id\"],\n",
    "    resource_group=config[\"resource_group\"],\n",
    "    location=config[\"location\"],\n",
    "    create_resource_group=False,\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "# Registrar el modelo\n",
    "mname = \"modelo\"\n",
    "registered_model = Model.register(\n",
    "    model_path=\"modelo.pkl\",\n",
    "    model_name=mname,\n",
    "    workspace=ws\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ score.py ###################\n",
    "scorepy = \"\"\"\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "  global model\n",
    "  model_path = Model.get_model_path('modelo')\n",
    "  model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "  try:\n",
    "    # Parse the input data\n",
    "    data = json.loads(raw_data)['data']\n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    required_columns = [\n",
    "      'Title_Mr.', 'Title_Ms.', 'Title_Sr.', 'Title_Sra.',\n",
    "      'SalesPerson_adventure-works\\\\david8', 'SalesPerson_adventure-works\\\\garrett1',\n",
    "      'SalesPerson_adventure-works\\\\jae0', 'SalesPerson_adventure-works\\\\jillian0',\n",
    "      'SalesPerson_adventure-works\\\\josé1', 'SalesPerson_adventure-works\\\\linda3',\n",
    "      'SalesPerson_adventure-works\\\\michael9', 'SalesPerson_adventure-works\\\\pamela0',\n",
    "      'SalesPerson_adventure-works\\\\shu0'\n",
    "    ]\n",
    "    \n",
    "    # Check if we received raw or encoded data\n",
    "    if 'Title' in data.columns and 'SalesPerson' in data.columns:\n",
    "      # Create dummy variables - matching the training format exactly\n",
    "      title_dummies = pd.get_dummies(data['Title'], prefix='Title')\n",
    "      salesperson_dummies = pd.get_dummies(data['SalesPerson'], prefix='SalesPerson')\n",
    "      \n",
    "      # Make sure all expected columns are present\n",
    "      for col in required_columns:\n",
    "        prefix = col.split('_')[0]\n",
    "        value = '_'.join(col.split('_')[1:])\n",
    "        \n",
    "        if prefix == 'Title':\n",
    "          if col not in title_dummies:\n",
    "            title_dummies[col] = 0\n",
    "        elif prefix == 'SalesPerson':\n",
    "          if col not in salesperson_dummies:\n",
    "            salesperson_dummies[col] = 0\n",
    "      \n",
    "      # Combine the data frames\n",
    "      data_encoded = pd.concat([title_dummies, salesperson_dummies], axis=1)\n",
    "      \n",
    "      # Only keep the columns used during training\n",
    "      data_encoded = data_encoded[required_columns]\n",
    "      \n",
    "    elif all(col in data.columns for col in required_columns):\n",
    "      # We received already encoded data\n",
    "      data_encoded = data[required_columns]\n",
    "    else:\n",
    "      missing_cols = [col for col in required_columns if col not in data.columns]\n",
    "      return json.dumps(f\"Missing required columns: {', '.join(missing_cols[:5])}...\")\n",
    "    \n",
    "    # Make the prediction\n",
    "    result = model.predict(data_encoded).tolist()\n",
    "    return json.dumps(result)\n",
    "    \n",
    "  except Exception as e:\n",
    "    return json.dumps(str(e))\n",
    "\"\"\"\n",
    "\n",
    "file_score = open(\"score.py\", \"w\")\n",
    "file_score.write(scorepy)\n",
    "file_score.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/6d3jsq3n12j27vfn8yqh_y500000gn/T/ipykernel_29056/381904218.py:18: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  service = Model.deploy(workspace=ws,\n"
     ]
    }
   ],
   "source": [
    "## STEP 2: Inference configuration: the blueprints for the German carpenter about how to build your furniture.\n",
    "\n",
    "from azureml.core.environment import Environment\n",
    "virtual_env = Environment(\"env-4-housing\")\n",
    "\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "virtual_env.python.conda_dependencies = CondaDependencies.create(conda_packages=['pandas', 'scikit-learn', 'numpy', 'joblib']) ## Asegúrense de coordinarse con el departamento de modelos para incluir los paquetes correctos. Si no, BOOM!\n",
    "\n",
    "\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "inference_config = InferenceConfig(\n",
    "                                environment=virtual_env,\n",
    "                                entry_script=\"score.py\",\n",
    "                                )\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=0.5, memory_gb=1) ## ASEGÚRENSE DE ASIGNAR SUFICIENTE MADERA PARA SUS MUEBLES.\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name='service',\n",
    "                       models=[registered_model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=aci_config,\n",
    "                       overwrite=True,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "service.wait_for_deployment()\n",
    "\n",
    "scoring_uri = service.scoring_uri\n",
    "\n",
    "scoreuri = json.dumps({\"URI\": [scoring_uri]})\n",
    "file = open(\"uri.json\", \"w\")\n",
    "file.write(scoreuri)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
